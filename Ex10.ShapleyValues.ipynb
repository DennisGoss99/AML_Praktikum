{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10th exercise: <font color=\"#C70039\">Interpretable Machine Learning by means of Shapley Values</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Date:   08.08.2023\n",
    "\n",
    "<img src=\"https://shap.readthedocs.io/en/latest/_images/example_notebooks_overviews_An_introduction_to_explainable_AI_with_Shapley_values_13_0.png\" style=\"float: center;\" width=\"600\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "\n",
    "Before using Shapley values to explain complicated models, it is helpful to understand how they work for simple models.\n",
    "\n",
    "In this respect the example in this notebook computes a model for the titanic data set (downloaded from Kaggle) and uses its outputs for explanation of feature importance using SHAP deepexplainer. In addition, several different visualization techniques (plots) for Shapley values are going to be demonstrated. \n",
    "\n",
    "For a description of the features please refer to <a href=\"https://www.kaggle.com/competitions/titanic/data\">Kaggle Titanic data set</a>.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "4. Develop a CNN for image classification and adapt the Shapley Value idea to that model. Comment your entire code.  \n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1MnrZSd06ED"
   },
   "source": [
    "## Imports\n",
    "Import all necessary python utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import os\n",
    "%matplotlib inline\n",
    "import shap\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load build-in dataset\n",
    "take the titanic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./data/titanic/')\n",
    "train_data = pd.read_csv('./train.csv', index_col=0)\n",
    "test_data = pd.read_csv('./test.csv', index_col=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Since the titanic data is a raw data set there is a need to preprocess it by dropping unnecessary columns, handling missing data, converting categorical features to numeric features and conducting one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    \n",
    "    df = df.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
    "    \n",
    "    # fill na\n",
    "    df[['Age']] = df[['Age']].fillna(value=df[['Age']].mean())\n",
    "    df[['Embarked']] = df[['Embarked']].fillna(value=df['Embarked'].value_counts().idxmax())\n",
    "    df[['Fare']] = df[['Fare']].fillna(value=df[['Fare']].mean())\n",
    "    \n",
    "    # encode categorical features into numeric\n",
    "    df['Sex'] = df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    embarked_one_hot = pd.get_dummies(df['Embarked'], prefix='Embarked')\n",
    "    \n",
    "    df = df.drop('Embarked', axis=1)\n",
    "    df = df.join(embarked_one_hot)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# training data processing\n",
    "train_data = data_preprocessing(train_data)\n",
    "train_data.isnull().sum()\n",
    "\n",
    "# create data for training\n",
    "x_train = train_data.drop(['Survived'], axis=1).values\n",
    "\n",
    "# Check testing data\n",
    "test_data.isnull().sum()\n",
    "\n",
    "# normalize training data\n",
    "scale = StandardScaler()\n",
    "x_train = scale.fit_transform(x_train)\n",
    "\n",
    "# prepare y_train\n",
    "y_train = train_data['Survived'].values\n",
    "\n",
    "# preprocess testing data\n",
    "test_data = data_preprocessing(test_data)\n",
    "x_test = test_data.values.astype(float)\n",
    "\n",
    "# normalize testing data\n",
    "x_test = scale.transform(x_test)\n",
    "\n",
    "# Check testing data\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple vanilla ANN, compile and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, compute the Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, x_train)\n",
    "shap_values = explainer.shap_values(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley values interpretation \n",
    "#### Global interpretation method\n",
    "\n",
    "The summary plot shows the most important features and the magnitude of their impact on the model. It is the global interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], plot_type = 'bar', feature_names = test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Interpretation methods\n",
    "##### Force plot\n",
    "The force plot is great for seeing where the “output value” fits in relation to the “base value”. \n",
    "Further, it is possible to observe which features have a positive (red) or negative (blue) impact on the prediction and in addition the magnitude of the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0].numpy(), shap_values[0][0], features = test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision plot\n",
    "The decision plot enables to observe the amplitude of each change taken by a sample for the values of the displayed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(explainer.expected_value[0].numpy(), shap_values[0][0], features = test_data.iloc[0,:], \n",
    "                   feature_names = test_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Waterfall plot\n",
    "The waterfall plot allows for seeing the amplitude and the nature of the impact of a feature. \n",
    "It also allows for seeing the order of importance of the features and the values taken by each feature for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots._waterfall.waterfall_legacy(explainer.expected_value[0].numpy(), shap_values[0][0], \n",
    "                                       feature_names = test_data.columns)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lime_image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
